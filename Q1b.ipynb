{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section A, Part 2:\n",
    "The tag wise and pos wise accuracies have been attached for each file. However, this may not cover the whole picture. I have used dictionaries to make my best estimate of the accuracies in ner and pos. In the POS tags, There were slight differences in length of the produced tags. Hence, comparing these directly would skew the accuracy highly due to a few outliers in between. Hence, this was not possible. For NER as well, I have attempted to replace the available tags to resemble the original dataset best. However, my assumptions are not necessarily perfect, but the logic still stands. These play a huge role in the accuracy that came out. For example, named entities in the assigned tags that did not have a direct replacement have not been counted. I have devised my own metic (but uncommenting the lines  where accuracy is counted would factor those tags in too). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section A, Part 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the tags, spacy took 3 minutes, nltk took 6 minutes while stanfordner took hours. \n",
    "\n",
    "From my research, I have observed that nltk is a complex algorithm to process nlp functions for research while spacy acts as a service. Stanfordner is an even more fine tuned version of spacy. Hence, spacy prioritises efficiency (even while downloading, there were different versions for efficiency and accuracy) and gives least importance to accuracy. NLTK is essentially a string processing library, where each function takes strings as input and returns a processed string. Spacy uses objects. \n",
    "\n",
    "Spacy is the most time efficient, however during the process execution I observed that it took up the highest ram space. \n",
    "\n",
    "The pattern I observed is that accuracy and efficiency are tradeoffs to each other, even space consumption to a certain extent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also observed the difference in the tags generated by each library. I have used dictionaries to make these conversions to compare this. For example, Stanfordner used the O tag when a certain word was not a named entity for the library like the labelled dataset. However, Spacy and nltk did not use this. This could be for efficiency. Our libraries did not use B- and I- tags like the labelled dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
